---
layout: post
mathjax: true
title: Neural networks meet Projection pursuit and Latent variable regression
---
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" >
</script> 

Neural networks have been hitting the ball out of the park for a variety of machine learning problems in pattern recognition, computer vision, natural language processing and time series forecasting. A key concept in deep learning with neural networks has been the discovery of hidden or latent features in the input data, which also lend themselves to interpretation. 
In this article, we will look at the application of neural nets to the classical nonlinear regression problem, and look at one architecture that lets us bring (somewhat) similar feature discovery and interpretability to this problem.
Of course, neural networks have been applied many times to such a setting, but here we will see an interesting (at least to me) approach that combines concepts from residual learning, projection pursuit and latent variable regression to give us an interpretable network architecture. It also gives us some nice by-products such as global sensitivities and a controllable low-dimensional projection of the input feature space that is chosen to best represent the influence on the dependent variable.

# The problem
Consider the standard regression problem, where we are given a sample set of $$s$$-dimensional vectors $$\mathbf{x}_i$$ from an $$s$$-dimensional input space and corresponding values $$y_i$$ of a dependent variable, and wish to generate a function $$\hat{y} = f(\mathbf{x})$$ such that the prediction errors between $$\hat{y}_i = f(\mathbf{x}_i)$$ and $$y_i$$ are minimized in some sense.
Here we will minimize the sum of squared errors
$$
\sum_{i=0}^n (y_i - \hat{y}_i)^2.
$$

Apart from just obtaining some function $$f()$$ that minimizes this error, it is often desired to have $$f()$$ be interpretable in some way so as to reveal the structure of the relationship captured by it. One aspect of this structure that is often quite revealing is its true dimensionality. There may be $$s$$ (e.g. 10) dimensions in the input space, but it may be that the dependent variable is only significantly influenced by a small subset of those dimensions. This is what we typically refer to as predictor importance. A more interesting question is whether there is a lower dimensional subspace of the input space that would largely explain the variation of the dependent variable.

This figure shows a simple example with 2 input dimensions, where the dependent variable is varying only along the direction indicated by the green dashed arrow. This direction aligns with the vector $$(x_1,x_2) = (1,2)$$. In this case the lower dimensional subspace is the 1-D subspace defined by the vector $$(1,2)$$.

![2D 1LV example]({{ site.baseurl }}/images/2var1lv.png "2-D input space with 1 latent variable that explains all the variation in the dependent variable.")

Let us define a variable $$t$$ along this direction. $$t$$ can be considered as a hidden or _latent variable_. This latent variable is defined by the vector $$(x_1,x_2) = (1,2)$$ in the original input space. This means that it is more closely aligned to $$x2$$ than to $$x_1$$ by a factor of 2. Since the dependent variable shows all its variation along this latent variable direction, we can infer that in some global sense $$x_2$$ has twice the influence on $$y$$ compared to the influence of $$x_1$$. In one extreme case, if the latent variable direction was defined by $$(x_1,x_2) = (0,2)$$, then $$x_1$$ would have no influence on $$y$$ at all.

Of course, in the general setting we may have more that 1 latent variable required to explain most, if not all, the variance in the dependent variable. Apart from solving the basic regression problem of minimizing the prediction errors, SiLVR lets us discover these latent variables, while at the same time determining the component of $$y$$ that varying along each latent variable direction. We will see how it does this in the following sections.

# The architecture
## Typical regression network
Let us consider a vanilla fully connected network with 1 hidden layer, as shown in the following figure. This is a common first pick for regression using neural networks.

![3LP]({{ site.baseurl }}/images/3lpinset.png "A 3-layer regression neuron.")

Each node in the hidden layer has an activation function given by
$$
g_i = \sigma(\mathbf{w}_i^T\mathbf{x} + b_i),
$$
where $$\sigma(\dot)$$ is a nonlinearity, often a sigmoidal function.
The dot product $$\mathbf{w}_i^T\mathbf{x}$$ is projecting the input vector along the weight vector $$\mathbf{w}_i$$. This projected value is then translated by the bias $$b_i$$ after which the non-linearity is applied.
$$g_i$$ is then a sigmoidal function that is aligned along the direction of the vector $$\mathbf{w}_i$$.
The output layer is then performing a linear combination of the activations from the hidden layer, as in $$\hat{y} = \sum_{i=1}^m d_ig_i + e = \mathbf{d}^T\mathbf{g} + e$$, where $$d_i$$ are the weights and $$e$$ is the bias term.
Each edge in the network implies a parameter. The dangling input edges of the hidden nodes indicate the biases ($$b_i, e$$).

## The SiLVR network
Let us now make a couple of key changes to the typical network architecture, with the goal of making the network parameters more interpretable:
1. Replace each hidden node with a small sub-network, which we will call a _ridge stack_, and
2. Make the width of the hidden layer dynamic: add ridge stacks only as needed.
Let us see how this helps.

The ridge stack architecture is shown in this figure:
![ridgestack]({{ site.baseurl}}/images/silvr_ridgestack.png "A ridge stack.")
It has three layers:
1. Projection: This layer simply projects the input vector $$\mathbf{x}$$ onto the weight vector $$\mathbf{w}_i$$ via a dot product which is denoted by the variable $$t_i$$.
2. Nonlinearity: This $$t_i$$ is then taken through $$q$$ nodes in parallel, each of which scales $$t_i$$ by a factor $$b_{i,j}$$, translates it by adding a bias of $$c_{i,j}$$ and then applies a sigmoidal nonlinearity, as in $$h_{i,j} = \sigma(b_{i,j}t_i + c_{i,j})$$. Each of these sigmoids is aligned along the direction of the projection vector (weight vector) $$\mathbf{w}_i$$, but each stretched, translated and sometimes flipped in different ways, as needed by the network.
3. Composition: Compute a weighted sum of the activations from the nonlinearity layer, with a bias, to give the output of the ridge stack: $$g_i = \mathbf{a}_i^T\mathbf{h}_i + d_i$$. This layer puts all the transformed sigmoids together to create a complex (or simple) nonlinear function that varies only along the projection vector. This nonlinear function can take a variety of different shapes, for instance from a ramp, a staircase curve, a peaky curve, a combination of staircase and peaks, and so on. 

The ridge stack is essentially trying to identify, with the projection / weight vector $$\mathbf{w}_i$$, the latent variable direction along which most of the variation in the dependent variable $$y$$ occurs. At the same time it is also trying to fit the curve $$g_i(t_i)$$ to best match the component of $$y$$ that is varying along this direction.

The term ridge stack is inspired from the concept of ridge functions. A ridge function is simply a function that varies only along one direction in the input space. It is of the general form $$F(\mathbf{x}) = f(\mathbf{a}^T\mathbf{x})$$, where $$\mathbf{a}$$ is a fixed vector and $$f$$ is some univariate function. If we consider a 2-dimensional input space then a ridge function would be contant along one direction (orthogonal to the direction of variation), leading to "ridges" in the topology. A simple example is in this figure, where we can see the ridge in the direction perpendicular to the direction of variation marked by the black arrow.

![2Dridgesurface]({{ site.baseurl }}/images/RidgeExample.png "A 2D ridge function that varies along the direction marked by the black arrow.")

The parameters of the ridge stack are the elements of the vectors $$\mathbf{w}_i, \mathbf{a}_i, \mathbf{b}_i$$ and $$\mathbf{c}_i$$ and the scalar $$d_i$$. We will optimize these parameters to minimize the fitting error between the dependent variable and the output of the stack $$g_i$$. $$\mathbf{w}_i$$ will then be the latent variable direction along which most of the variation of the dependent variable manifests and $$g_i(t_i) = g_i(\mathbf{w}_i^T\mathbf{x})$$ will be the ridge function that best matches this variation (or shape) of the dependent variable along this direction.

The full network architecture using ridge stacks looks like this:


# By-products
## Global sensitivities
## Input-referred correlation
## Visualization

# Training

# References
