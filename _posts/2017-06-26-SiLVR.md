---
layout: post
mathjax: true
title: Neural networks meet Projection pursuit and Latent variable regression
---
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" >
</script> 

Neural networks have been hitting the ball out of the park for a variety of machine learning problems in pattern recognition, computer vision, natural language processing and time series forecasting. In this article, I describe an application of neural nets to the classical regression problem, particularly when we have to deal with high dimensionality and nonlinearity at the same time. Of course, neural networks have been applied many times to such a setting, but here we will see an interesting (at least to me) approach that combines concepts from residual learning, projection pursuit and latent variable regression to give us an interpretable network architecture. It also gives us some nice by-products such as global sensitivities and a controllable low-dimensional projection of the input feature space that is chosen to best represent the influence on the dependent variable.

# The problem
Consider the standard regression problem, where we are given a sample set of vectors $$ x_i $$ from an input space and corresponding values $$y_i$$ of a dependent variable, and wish to generate a function $\hat{y} = f(x)$ such that the errors between $$\hat{y}_i = f(x_i}$$ and $y_i$ are minimized in some sense.

# The architecture

# By-products
## Global sensitivities
## Input-referred correlation
## Visualization

# Training

# References
